{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Libraries**\n",
        "\n"
      ],
      "metadata": {
        "id": "3AVFk89sVMJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers peft torch accelerate bitsandbytes\n"
      ],
      "metadata": {
        "id": "hN6jFqa-VOXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download Google drive folder with trained model data**"
      ],
      "metadata": {
        "id": "9ZR5KIMwVmfT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thOPlSm3BVuN",
        "outputId": "e3b6b9d3-88f4-433a-a1ce-69f89e4fb2a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading your trained model files from Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1ufkk-Pok1Jj27YxjWSmD92pXrC5DKv1q adapter_config.json\n",
            "Processing file 1TyESkqVSppq_w1MROrBxQvXghQSNtWJ1 adapter_model.safetensors\n",
            "Processing file 1Tdxku1odefxYlIzr_NBf0IImK26iSV_C sample_training_data.json\n",
            "Processing file 1CxtjWrFArCUY2-Jnn_QinsX_tOYoPyAD special_tokens_map.json\n",
            "Processing file 1SEt3zkMfSpB969JwEbJhTmefGRNp5Iij tokenizer_config.json\n",
            "Processing file 16l-plbzWhdXhJSPB1eSvnD7YS-vsSkrZ tokenizer.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ufkk-Pok1Jj27YxjWSmD92pXrC5DKv1q\n",
            "To: /content/model_files/model_files/QA_finetuning_test/adapter_config.json\n",
            "100%|██████████| 765/765 [00:00<00:00, 2.98MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TyESkqVSppq_w1MROrBxQvXghQSNtWJ1\n",
            "From (redirected): https://drive.google.com/uc?id=1TyESkqVSppq_w1MROrBxQvXghQSNtWJ1&confirm=t&uuid=2fd11ce8-1410-47f6-bf9c-4fc3ed9700ad\n",
            "To: /content/model_files/model_files/QA_finetuning_test/adapter_model.safetensors\n",
            "100%|██████████| 446M/446M [00:08<00:00, 54.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Tdxku1odefxYlIzr_NBf0IImK26iSV_C\n",
            "To: /content/model_files/model_files/QA_finetuning_test/sample_training_data.json\n",
            "100%|██████████| 20.9k/20.9k [00:00<00:00, 32.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CxtjWrFArCUY2-Jnn_QinsX_tOYoPyAD\n",
            "To: /content/model_files/model_files/QA_finetuning_test/special_tokens_map.json\n",
            "100%|██████████| 463/463 [00:00<00:00, 1.54MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SEt3zkMfSpB969JwEbJhTmefGRNp5Iij\n",
            "To: /content/model_files/model_files/QA_finetuning_test/tokenizer_config.json\n",
            "100%|██████████| 19.0k/19.0k [00:00<00:00, 28.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16l-plbzWhdXhJSPB1eSvnD7YS-vsSkrZ\n",
            "To: /content/model_files/model_files/QA_finetuning_test/tokenizer.json\n",
            "100%|██████████| 7.15M/7.15M [00:00<00:00, 30.6MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/model_files/model_files/QA_finetuning_test/adapter_config.json',\n",
              " '/content/model_files/model_files/QA_finetuning_test/adapter_model.safetensors',\n",
              " '/content/model_files/model_files/QA_finetuning_test/sample_training_data.json',\n",
              " '/content/model_files/model_files/QA_finetuning_test/special_tokens_map.json',\n",
              " '/content/model_files/model_files/QA_finetuning_test/tokenizer_config.json',\n",
              " '/content/model_files/model_files/QA_finetuning_test/tokenizer.json']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import gdown\n",
        "import os\n",
        "\n",
        "os.makedirs(\"model_files\", exist_ok=True)\n",
        "os.chdir(\"model_files\")\n",
        "\n",
        "print(\"Downloading your trained model files from Google Drive...\")\n",
        "\n",
        "folder_url = \"https://drive.google.com/drive/folders/1AKzzA2WObNmontbDVaAY8ZPNF7g669Mr\"\n",
        "\n",
        "gdown.download_folder(folder_url, quiet=False, use_cookies=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check downloaded files**"
      ],
      "metadata": {
        "id": "vxUKTc6KVv6a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9yJjzi9GQaP",
        "outputId": "47dd9204-4650-4067-ba76-7752adbdc609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloaded files:\n",
            "  ./QA_finetuning_test/adapter_model.safetensors: 425.0 MB\n",
            "  ./QA_finetuning_test/special_tokens_map.json: 0.0 MB\n",
            "  ./QA_finetuning_test/sample_training_data.json: 0.0 MB\n",
            "  ./QA_finetuning_test/adapter_config.json: 0.0 MB\n",
            "  ./QA_finetuning_test/tokenizer.json: 6.8 MB\n",
            "  ./QA_finetuning_test/tokenizer_config.json: 0.0 MB\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nDownloaded files:\")\n",
        "for root, dirs, files in os.walk(\".\"):\n",
        "    for file in files:\n",
        "        filepath = os.path.join(root, file)\n",
        "        size = os.path.getsize(filepath) / (1024*1024)  # Size in MB\n",
        "        print(f\"  {filepath}: {size:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change runtime type if needed** \\\\\n",
        "Loading the model will need higher RAM"
      ],
      "metadata": {
        "id": "0OzWR8xMV0OP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIwzJWytBV6-",
        "outputId": "fb7fd25d-2987-4d86-9d46-a16b8347bafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU device: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load trained Phi4 model** \\\\\n",
        "trained on 34 files from Bohan's SOS dataset"
      ],
      "metadata": {
        "id": "zB67rxQdWA4q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798,
          "referenced_widgets": [
            "9b4ed32562094847a4bfb846b2e97926",
            "d56575a66f104e5da1740de6b026fed6",
            "222425702b574367a00ab97edab4a0e7",
            "b0a1d08807cf41e08595d91386f2a099",
            "07e6782560e145d19086533fbca971f5",
            "5527145bca9f47e7ba59e393b1ed1cda",
            "45e832e002d34b15a43258caada234f8",
            "f55c1e25e8d740bf90b55211b1c8af9c",
            "104b50e44d2e4554a204827e82e1307a",
            "d5e8a74c647042f380e78408df7f806f",
            "8abed4c08c554473a0367cbffb562da7"
          ]
        },
        "id": "1UKaMF02BV-G",
        "outputId": "b822b32d-3b9e-475b-895b-e34831a2920b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Adapter config loaded:\n",
            "  alpha_pattern: {}\n",
            "  auto_mapping: None\n",
            "  base_model_name_or_path: microsoft/Phi-4-reasoning-plus\n",
            "  bias: none\n",
            "  eva_config: None\n",
            "  exclude_modules: None\n",
            "  fan_in_fan_out: None\n",
            "  inference_mode: True\n",
            "  init_lora_weights: True\n",
            "  layer_replication: None\n",
            "  layers_pattern: None\n",
            "  layers_to_transform: None\n",
            "  loftq_config: {}\n",
            "  lora_alpha: 16\n",
            "  lora_bias: False\n",
            "  lora_dropout: 0.05\n",
            "  megatron_config: None\n",
            "  megatron_core: megatron.core\n",
            "  modules_to_save: None\n",
            "  peft_type: LORA\n",
            "  r: 32\n",
            "  rank_pattern: {}\n",
            "  revision: None\n",
            "  target_modules: ['o_proj', 'down_proj', 'qkv_proj', 'gate_up_proj']\n",
            "  task_type: CAUSAL_LM\n",
            "  use_dora: False\n",
            "  use_rslora: False\n",
            "\n",
            "Files in adapter directory:\n",
            "  adapter_model.safetensors: 425.0 MB\n",
            "  special_tokens_map.json: 0.0 MB\n",
            "  sample_training_data.json: 0.0 MB\n",
            "  adapter_config.json: 0.0 MB\n",
            "  tokenizer.json: 6.8 MB\n",
            "  tokenizer_config.json: 0.0 MB\n",
            "\n",
            " Loading fine-tuned Phi-4 model...\n",
            "Loading from adapter path: ./QA_finetuning_test/\n",
            "Loading tokenizer...\n",
            "Loading base model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b4ed32562094847a4bfb846b2e97926"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: CUDA out of memory. Tried to allocate 176.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 150.88 MiB is free. Process 16420 has 39.40 GiB memory in use. Of the allocated memory 38.78 GiB is allocated by PyTorch, and 123.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "import json\n",
        "import random\n",
        "\n",
        "def load_model_with_fallback():\n",
        "    base_model_name = \"microsoft/Phi-4-reasoning-plus\"\n",
        "    adapter_path = \"./QA_finetuning_test/\"\n",
        "\n",
        "    print(f\"Loading from adapter path: {adapter_path}\")\n",
        "\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "    print(\"Loading base model...\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        base_model = base_model.to(\"cuda\")\n",
        "        print(\"Base model moved to GPU.\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Method 1: Standard loading\n",
        "        print(\"  Trying standard PEFT loading...\")\n",
        "        model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "        print(\"Standard loading successful!\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e1:\n",
        "        print(f\"  Standard loading failed: {str(e1)[:100]}...\")\n",
        "\n",
        "        try:\n",
        "            # Method 2: Load with specific config\n",
        "            print(\"  Trying with explicit config...\")\n",
        "            peft_config = PeftConfig.from_pretrained(adapter_path)\n",
        "            model = PeftModel.from_pretrained(\n",
        "                base_model,\n",
        "                adapter_path,\n",
        "                config=peft_config,\n",
        "                is_trainable=False\n",
        "            )\n",
        "            print(\"Config-based loading successful!\")\n",
        "            return model, tokenizer\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"  Config loading failed: {str(e2)[:100]}...\")\n",
        "\n",
        "            try:\n",
        "                # Method 3: Force loading with adapter_name\n",
        "                print(\"  Trying with default adapter name...\")\n",
        "                model = PeftModel.from_pretrained(\n",
        "                    base_model,\n",
        "                    adapter_path,\n",
        "                    adapter_name=\"default\"\n",
        "                )\n",
        "                print(\"Named adapter loading successful!\")\n",
        "                return model, tokenizer\n",
        "\n",
        "            except Exception as e3:\n",
        "                print(f\"  Named loading failed: {str(e3)[:100]}...\")\n",
        "                return base_model, tokenizer\n",
        "\n",
        "def inspect_adapter_files():\n",
        "    adapter_path = \"./QA_finetuning_test/\"\n",
        "\n",
        "    try:\n",
        "        with open(f\"{adapter_path}/adapter_config.json\", 'r') as f:\n",
        "            config = json.load(f)\n",
        "        print(f\" Adapter config loaded:\")\n",
        "        for key, value in config.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error reading adapter config: {e}\")\n",
        "\n",
        "    import os\n",
        "    print(f\"\\nFiles in adapter directory:\")\n",
        "    for file in os.listdir(adapter_path):\n",
        "        size = os.path.getsize(f\"{adapter_path}/{file}\") / (1024*1024)\n",
        "        print(f\"  {file}: {size:.1f} MB\")\n",
        "\n",
        "inspect_adapter_files()\n",
        "\n",
        "print(\"\\n Loading fine-tuned Phi-4 model...\")\n",
        "try:\n",
        "    model, tokenizer = load_model_with_fallback()\n",
        "    model_loaded = True\n",
        "    print(\" Model loading completed!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    model_loaded = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test with sample conversations from dataset** \\\\\n",
        "Resulting metric is a success rate of the answers \\\\\n",
        "Results saved to `adapter_test_results.json`"
      ],
      "metadata": {
        "id": "Lfd1QpL7Wf9q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "29FtTSyM7x9e",
        "outputId": "a43a7aeb-99b9-46c4-d743-0158c7e82cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading questions from ./QA_finetuning_test/sample_training_data.json...\n",
            "Found 5 conversations in sample data\n",
            " Extracted 5 questions from sample training data\n",
            "\n",
            " Testing model with 5 questions...\n",
            "================================================================================\n",
            "\n",
            " QUESTION 1:\n",
            "Consider the following constraints:\n",
            "g_1 = (z - 2) >= 0\n",
            "g_2 = (y + 2*z - 3) >= 0\n",
            "g_3 = (2*y + z) >= 0\n",
            "g_4 = (y^2*z^2 - 2*y*z - 2) >= 0\n",
            "g_5 = (z + 2) >=...\n",
            "\n",
            " GENERATING ANSWER...\n",
            "GENERATED: y*z^3 + 6*y*z^2 + 12*y*z + 8*y + 3*y + 2*z^4 + 9*z^3 + 6*z^2 - 20*z + 6*z - 24 - 9\n",
            " ORIGINAL:  y*z^3 + 6*y*z^2 + 12*y*z + 8*y + 3*y + 2*z^4 + 9*z^3 + 6*z^2 - 20*z + 6*z - 24 - 9...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " QUESTION 2:\n",
            "Consider the following constraints:\n",
            "g_1 = (x*z + 2) >= 0\n",
            "g_2 = (z + 1) >= 0\n",
            "g_3 = (z*(x - 3)) >= 0\n",
            "\n",
            "Consider the target polynomial:\n",
            "f = 8*x^6*z^3 + 8*...\n",
            "\n",
            " GENERATING ANSWER...\n",
            "GENERATED: 8*x^6*z^3 + 8*x^6*z^2 - 8*x^5*z^4 - 8*x^5*z^3 - 48*x^5*z^2 - 48*x^5*z + 2*x^4*z^5 + 2*x^4*z^4 + 48*x^4*z^3 + 48*x^4*z^2 + 72*x^4*z + 72*x^4 + 5*x^3*z^8 + 25*x^3*z^7 + 40*x^3*z^6 + 20*x^3*z^5 - 12*x^3*z^4 - 12*x^3\n",
            " ORIGINAL:  8*x^6*z^3 + 8*x^6*z^2 - 8*x^5*z^4 - 8*x^5*z^3 - 48*x^5*z^2 - 48*x^5*z + 2*x^4*z^5 + 2*x^4*z^4 + 48*x^4*z^3 + 48*x^4*z^2 + 72*x^4*z + 72*x^4 + 5*x^3*z^...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " QUESTION 3:\n",
            "Consider the following constraints:\n",
            "g_1 = (3*x^2 + 2*x + 2) >= 0\n",
            "g_2 = (4*z^2) >= 0\n",
            "g_3 = (x^2*z + x - 2) >= 0\n",
            "\n",
            "Consider the target polynomial:\n",
            "f = 2*...\n",
            "\n",
            " GENERATING ANSWER...\n",
            "GENERATED: 2*x^4*z^2 + 4*x^3*z - 8*x^2*z + 2*x^2 - 8*x + 8\n",
            " ORIGINAL:  2*x^4*z^2 + 4*x^3*z - 8*x^2*z + 2*x^2 - 8*x + 8...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " QUESTION 4:\n",
            "Consider the following constraints:\n",
            "\n",
            "\n",
            "Consider the target polynomial:\n",
            "f = 3*y^12 - 6*y^10 - 3*y^8 + 12*y^6 + 2*y^4 + 8*y^3 + 4\n",
            "\n",
            "Our objective is to re...\n",
            "\n",
            " GENERATING ANSWER...\n",
            "GENERATED: 3*y^12 - 6*y^10 - 3*y^8 + 12*y^6 + 2*y^4 + 8*y^3 + 4\n",
            " ORIGINAL:  3*y^12 - 6*y^10 - 3*y^8 + 12*y^6 + y^4 + 4*y^4 - 3*y^4 + 8*y^3 + 2*y^2 + 4*y^2 - 6*y^2 + 1 + 3...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " QUESTION 5:\n",
            "Consider the following constraints:\n",
            "g_1 = ((x + 3)^2) >= 0\n",
            "g_2 = (1 - 2*x) >= 0\n",
            "\n",
            "Consider the target polynomial:\n",
            "f = 5 - 10*x\n",
            "\n",
            "Our objective is to rew...\n",
            "\n",
            " GENERATING ANSWER...\n",
            "GENERATED: -10*x + 5\n",
            " ORIGINAL:  -10*x + 5...\n",
            "--------------------------------------------------------------------------------\n",
            "Success rate: 5/5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fdf9be47-73c0-4881-9049-95e2cb9f43d8\", \"adapter_test_results.json\", 5888)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "def load_questions_from_sample_data():\n",
        "    sample_file = \"./QA_finetuning_test/sample_training_data.json\"\n",
        "\n",
        "    print(f\" Loading questions from {sample_file}...\")\n",
        "\n",
        "    with open(sample_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"Found {len(data)} conversations in sample data\")\n",
        "\n",
        "    questions = []\n",
        "\n",
        "    for i, conversation in enumerate(data):\n",
        "        messages = conversation['messages']\n",
        "\n",
        "        for j, message in enumerate(messages):\n",
        "            if (message['role'] == 'user' and\n",
        "                len(message['content']) > 100 and\n",
        "                'Consider the following constraints' in message['content']):\n",
        "\n",
        "                original_answer = None\n",
        "                if j + 1 < len(messages) and messages[j + 1]['role'] == 'assistant':\n",
        "                    original_answer = messages[j + 1]['content']\n",
        "\n",
        "                questions.append({\n",
        "                    'conversation_id': i,\n",
        "                    'question': message['content'],\n",
        "                    'original_answer': original_answer\n",
        "                })\n",
        "                break\n",
        "\n",
        "    print(f\" Extracted {len(questions)} questions from sample training data\")\n",
        "    return questions\n",
        "\n",
        "if 'model_loaded' not in globals():\n",
        "    model_loaded = False\n",
        "    print(\"model_loaded variable not found, assuming model is not loaded.\")\n",
        "\n",
        "if model_loaded:\n",
        "    try:\n",
        "        test_questions = load_questions_from_sample_data()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading questions: {e}\")\n",
        "        test_questions = []\n",
        "else:\n",
        "    print(\"Model not loaded, skipping loading test questions.\")\n",
        "    test_questions = []\n",
        "\n",
        "\n",
        "def generate_answer(model, tokenizer, question):\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "You are an expert in symbolic computation and polynomial decomposition.\n",
        "Your task is to help rewrite a target polynomial into the required form based on given inequality premises.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{question}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.3,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id, # Added pad_token_id\n",
        "            eos_token_id=tokenizer.eos_token_id, # Added eos_token_id\n",
        "            repetition_penalty=1.1 # Added repetition_penalty\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    if \"<|im_start|>assistant\" in response:\n",
        "        answer = response.split(\"<|im_start|>assistant\")[-1].strip()\n",
        "    else:\n",
        "        answer = response.strip()\n",
        "\n",
        "    if answer.endswith(\"<|im_end|>\"):\n",
        "        answer = answer[:-len(\"<|im_end|>\")].strip()\n",
        "\n",
        "    return answer\n",
        "\n",
        "if not model_loaded:\n",
        "    print(\"Cannot run tests - model failed to load\")\n",
        "else:\n",
        "    results = []\n",
        "\n",
        "    num_tests = min(10, len(test_questions))\n",
        "    test_subset = test_questions[:num_tests]\n",
        "\n",
        "    print(f\"\\n Testing model with {num_tests} questions...\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    for i, q_data in enumerate(test_subset, 1):\n",
        "        question = q_data['question']\n",
        "        original_answer = q_data['original_answer']\n",
        "\n",
        "        print(f\"\\n QUESTION {i}:\")\n",
        "        print(f\"{question[:150]}...\")\n",
        "\n",
        "        print(f\"\\n GENERATING ANSWER...\")\n",
        "\n",
        "        try:\n",
        "            generated_answer = generate_answer(model, tokenizer, question)\n",
        "\n",
        "            print(f\"GENERATED: {generated_answer}\")\n",
        "            print(f\" ORIGINAL:  {original_answer[:150]}...\" if original_answer else \"N/A\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "            results.append({\n",
        "                \"question_id\": i,\n",
        "                \"question\": question,\n",
        "                \"generated_answer\": generated_answer,\n",
        "                \"original_answer\": original_answer,\n",
        "                \"status\": \"success\"\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Generation error: {str(e)}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "            results.append({\n",
        "                \"question_id\": i,\n",
        "                \"question\": question,\n",
        "                \"generated_answer\": f\"Error: {str(e)}\",\n",
        "                \"original_answer\": original_answer,\n",
        "                \"status\": \"error\"\n",
        "            })\n",
        "\n",
        "    if results:\n",
        "        with open('/content/adapter_test_results.json', 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "        successful_results = [r for r in results if r['status'] == 'success']\n",
        "        print(f\"Success rate: {len(successful_results)}/{len(results)}\")\n",
        "\n",
        "        from google.colab import files\n",
        "        try:\n",
        "            files.download('/content/adapter_test_results.json')\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading file: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b4ed32562094847a4bfb846b2e97926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d56575a66f104e5da1740de6b026fed6",
              "IPY_MODEL_222425702b574367a00ab97edab4a0e7",
              "IPY_MODEL_b0a1d08807cf41e08595d91386f2a099"
            ],
            "layout": "IPY_MODEL_07e6782560e145d19086533fbca971f5"
          }
        },
        "d56575a66f104e5da1740de6b026fed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5527145bca9f47e7ba59e393b1ed1cda",
            "placeholder": "​",
            "style": "IPY_MODEL_45e832e002d34b15a43258caada234f8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "222425702b574367a00ab97edab4a0e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f55c1e25e8d740bf90b55211b1c8af9c",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_104b50e44d2e4554a204827e82e1307a",
            "value": 6
          }
        },
        "b0a1d08807cf41e08595d91386f2a099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5e8a74c647042f380e78408df7f806f",
            "placeholder": "​",
            "style": "IPY_MODEL_8abed4c08c554473a0367cbffb562da7",
            "value": " 6/6 [00:49&lt;00:00,  9.73s/it]"
          }
        },
        "07e6782560e145d19086533fbca971f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5527145bca9f47e7ba59e393b1ed1cda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45e832e002d34b15a43258caada234f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f55c1e25e8d740bf90b55211b1c8af9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "104b50e44d2e4554a204827e82e1307a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5e8a74c647042f380e78408df7f806f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8abed4c08c554473a0367cbffb562da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
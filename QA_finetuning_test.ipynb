{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GKfEjgbN_pVg",
        "outputId": "7c44b22b-9141-4620-e528-8ec3f1d7f662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.46.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers peft torch accelerate bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import os\n",
        "\n",
        "os.makedirs(\"model_files\", exist_ok=True)\n",
        "os.chdir(\"model_files\")\n",
        "\n",
        "print(\"Downloading your trained model files from Google Drive...\")\n",
        "\n",
        "folder_url = \"https://drive.google.com/drive/folders/1AKzzA2WObNmontbDVaAY8ZPNF7g669Mr\"\n",
        "\n",
        "gdown.download_folder(folder_url, quiet=False, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thOPlSm3BVuN",
        "outputId": "ea0b30cd-3bee-4c30-9a3b-b43a214da6a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading your trained model files from Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1ufkk-Pok1Jj27YxjWSmD92pXrC5DKv1q adapter_config.json\n",
            "Processing file 1TyESkqVSppq_w1MROrBxQvXghQSNtWJ1 adapter_model.safetensors\n",
            "Processing file 1CvMpQvyBcuzhD7uD-stCtNxMZuo1ufzp sample_training_data.json\n",
            "Processing file 1CxtjWrFArCUY2-Jnn_QinsX_tOYoPyAD special_tokens_map.json\n",
            "Processing file 1SEt3zkMfSpB969JwEbJhTmefGRNp5Iij tokenizer_config.json\n",
            "Processing file 16l-plbzWhdXhJSPB1eSvnD7YS-vsSkrZ tokenizer.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ufkk-Pok1Jj27YxjWSmD92pXrC5DKv1q\n",
            "To: /content/model_files/model_files/QA_finetuning_test/adapter_config.json\n",
            "100%|██████████| 765/765 [00:00<00:00, 2.02MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TyESkqVSppq_w1MROrBxQvXghQSNtWJ1\n",
            "From (redirected): https://drive.google.com/uc?id=1TyESkqVSppq_w1MROrBxQvXghQSNtWJ1&confirm=t&uuid=f4618106-7e5a-4a76-9eab-91a86e72c49e\n",
            "To: /content/model_files/model_files/QA_finetuning_test/adapter_model.safetensors\n",
            "100%|██████████| 446M/446M [00:42<00:00, 10.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CvMpQvyBcuzhD7uD-stCtNxMZuo1ufzp\n",
            "To: /content/model_files/model_files/QA_finetuning_test/sample_training_data.json\n",
            "100%|██████████| 10.4k/10.4k [00:00<00:00, 4.70MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CxtjWrFArCUY2-Jnn_QinsX_tOYoPyAD\n",
            "To: /content/model_files/model_files/QA_finetuning_test/special_tokens_map.json\n",
            "100%|██████████| 463/463 [00:00<00:00, 1.74MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SEt3zkMfSpB969JwEbJhTmefGRNp5Iij\n",
            "To: /content/model_files/model_files/QA_finetuning_test/tokenizer_config.json\n",
            "100%|██████████| 19.0k/19.0k [00:00<00:00, 42.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16l-plbzWhdXhJSPB1eSvnD7YS-vsSkrZ\n",
            "To: /content/model_files/model_files/QA_finetuning_test/tokenizer.json\n",
            "100%|██████████| 7.15M/7.15M [00:00<00:00, 37.5MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/model_files/model_files/QA_finetuning_test/adapter_config.json',\n",
              " '/content/model_files/model_files/QA_finetuning_test/adapter_model.safetensors',\n",
              " '/content/model_files/model_files/QA_finetuning_test/sample_training_data.json',\n",
              " '/content/model_files/model_files/QA_finetuning_test/special_tokens_map.json',\n",
              " '/content/model_files/model_files/QA_finetuning_test/tokenizer_config.json',\n",
              " '/content/model_files/model_files/QA_finetuning_test/tokenizer.json']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDownloaded files:\")\n",
        "for root, dirs, files in os.walk(\".\"):\n",
        "    for file in files:\n",
        "        filepath = os.path.join(root, file)\n",
        "        size = os.path.getsize(filepath) / (1024*1024)  # Size in MB\n",
        "        print(f\"  {filepath}: {size:.1f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9yJjzi9GQaP",
        "outputId": "5c7d7197-4d07-4cd6-8a9b-dc90c5b27905"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloaded files:\n",
            "  ./QA_finetuning_test/sample_training_data.json: 0.0 MB\n",
            "  ./QA_finetuning_test/tokenizer_config.json: 0.0 MB\n",
            "  ./QA_finetuning_test/tokenizer.json: 6.8 MB\n",
            "  ./QA_finetuning_test/adapter_model.safetensors: 425.0 MB\n",
            "  ./QA_finetuning_test/adapter_config.json: 0.0 MB\n",
            "  ./QA_finetuning_test/special_tokens_map.json: 0.0 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIwzJWytBV6-",
        "outputId": "d54c98aa-0538-4bb0-b0df-ed9e7ad8f931"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "import json\n",
        "import random\n",
        "\n",
        "def load_model_with_fallback():\n",
        "    \"\"\"Load your fine-tuned model with fallback options\"\"\"\n",
        "    base_model_name = \"microsoft/Phi-4-reasoning-plus\"\n",
        "    adapter_path = \"./QA_finetuning_test/\"\n",
        "\n",
        "    print(f\"Loading from adapter path: {adapter_path}\")\n",
        "\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "    print(\"Loading base model...\")\n",
        "    # Load base model without device_map=\"auto\" initially\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Move base model to GPU explicitly\n",
        "    if torch.cuda.is_available():\n",
        "        base_model = base_model.to(\"cuda\")\n",
        "        print(\"Base model moved to GPU.\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Method 1: Standard loading\n",
        "        print(\"  Trying standard PEFT loading...\")\n",
        "        model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "        print(\"Standard loading successful!\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e1:\n",
        "        print(f\"  Standard loading failed: {str(e1)[:100]}...\")\n",
        "\n",
        "        try:\n",
        "            # Method 2: Load with specific config\n",
        "            print(\"  Trying with explicit config...\")\n",
        "            peft_config = PeftConfig.from_pretrained(adapter_path)\n",
        "            model = PeftModel.from_pretrained(\n",
        "                base_model,\n",
        "                adapter_path,\n",
        "                config=peft_config,\n",
        "                is_trainable=False\n",
        "            )\n",
        "            print(\"Config-based loading successful!\")\n",
        "            return model, tokenizer\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"  Config loading failed: {str(e2)[:100]}...\")\n",
        "\n",
        "            try:\n",
        "                # Method 3: Force loading with adapter_name\n",
        "                print(\"  Trying with default adapter name...\")\n",
        "                model = PeftModel.from_pretrained(\n",
        "                    base_model,\n",
        "                    adapter_path,\n",
        "                    adapter_name=\"default\"\n",
        "                )\n",
        "                print(\"Named adapter loading successful!\")\n",
        "                return model, tokenizer\n",
        "\n",
        "            except Exception as e3:\n",
        "                print(f\"  Named loading failed: {str(e3)[:100]}...\")\n",
        "\n",
        "                # Method 4: Just use base model (no fine-tuning)\n",
        "                print(\"  Using base model without fine-tuning...\")\n",
        "                print(\"  This will test the base Phi-4 model, not your trained version\")\n",
        "                return base_model, tokenizer\n",
        "\n",
        "def inspect_adapter_files():\n",
        "    adapter_path = \"./QA_finetuning_test/\"\n",
        "\n",
        "    print(\"\\n Inspecting adapter files:\")\n",
        "\n",
        "    try:\n",
        "        with open(f\"{adapter_path}/adapter_config.json\", 'r') as f:\n",
        "            config = json.load(f)\n",
        "        print(f\" Adapter config loaded:\")\n",
        "        for key, value in config.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error reading adapter config: {e}\")\n",
        "\n",
        "    import os\n",
        "    print(f\"\\n📁 Files in adapter directory:\")\n",
        "    for file in os.listdir(adapter_path):\n",
        "        size = os.path.getsize(f\"{adapter_path}/{file}\") / (1024*1024)\n",
        "        print(f\"  {file}: {size:.1f} MB\")\n",
        "\n",
        "inspect_adapter_files()\n",
        "\n",
        "print(\"\\n Loading your fine-tuned Phi-4 model...\")\n",
        "try:\n",
        "    model, tokenizer = load_model_with_fallback()\n",
        "    model_loaded = True\n",
        "    print(\" Model loading completed!\")\n",
        "except Exception as e:\n",
        "    print(f\"All loading methods failed: {e}\")\n",
        "    model_loaded = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "1UKaMF02BV-G",
        "outputId": "9f483412-4cf1-4170-9b07-9df756ab9c2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Inspecting adapter files:\n",
            " Error reading adapter config: [Errno 2] No such file or directory: './QA_finetuning_test//adapter_config.json'\n",
            "\n",
            "📁 Files in adapter directory:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './QA_finetuning_test/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-367931780.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  {file}: {size:.1f} MB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0minspect_adapter_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Loading your fine-tuned Phi-4 model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2-367931780.py\u001b[0m in \u001b[0;36minspect_adapter_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n📁 Files in adapter directory:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{adapter_path}/{file}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  {file}: {size:.1f} MB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './QA_finetuning_test/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_questions_from_sample_data():\n",
        "    sample_file = \"./QA_finetuning_test/sample_training_data.json\"\n",
        "\n",
        "    print(f\" Loading questions from {sample_file}...\")\n",
        "\n",
        "    with open(sample_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"Found {len(data)} conversations in sample data\")\n",
        "\n",
        "    questions = []\n",
        "\n",
        "    for i, conversation in enumerate(data):\n",
        "        messages = conversation['messages']\n",
        "\n",
        "        for j, message in enumerate(messages):\n",
        "            if (message['role'] == 'user' and\n",
        "                len(message['content']) > 100 and\n",
        "                'Consider the following constraints' in message['content']):\n",
        "\n",
        "                original_answer = None\n",
        "                if j + 1 < len(messages) and messages[j + 1]['role'] == 'assistant':\n",
        "                    original_answer = messages[j + 1]['content']\n",
        "\n",
        "                questions.append({\n",
        "                    'conversation_id': i,\n",
        "                    'question': message['content'],\n",
        "                    'original_answer': original_answer\n",
        "                })\n",
        "                break\n",
        "\n",
        "    print(f\" Extracted {len(questions)} questions from sample training data\")\n",
        "    return questions\n",
        "\n",
        "if model_loaded:\n",
        "    test_questions = load_questions_from_sample_data()\n",
        "\n",
        "def generate_answer(model, tokenizer, question):\n",
        "    \"\"\"Generate answer for a given question\"\"\"\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "You are an expert in symbolic computation and polynomial decomposition.\n",
        "Your task is to help rewrite a target polynomial into the required form based on given inequality premises.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{question}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,  # Reduced for faster generation\n",
        "            temperature=0.3,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = response.split(\"<|im_start|>assistant\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "if not model_loaded:\n",
        "    print(\"Cannot run tests - model failed to load\")\n",
        "else:\n",
        "    results = []\n",
        "\n",
        "    num_tests = min(2, len(test_questions))\n",
        "    test_subset = test_questions[:num_tests]\n",
        "\n",
        "    print(f\"\\n Testing model with {num_tests} questions...\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    for i, q_data in enumerate(test_subset, 1):\n",
        "        question = q_data['question']\n",
        "        original_answer = q_data['original_answer']\n",
        "\n",
        "        print(f\"\\n QUESTION {i}:\")\n",
        "        print(f\"{question[:150]}...\")\n",
        "\n",
        "        print(f\"\\n GENERATING ANSWER...\")\n",
        "\n",
        "        try:\n",
        "            generated_answer = generate_answer(model, tokenizer, question)\n",
        "\n",
        "            print(f\"GENERATED: {generated_answer}\")\n",
        "            print(f\" ORIGINAL:  {original_answer[:150]}...\" if original_answer else \"N/A\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "            results.append({\n",
        "                \"question_id\": i,\n",
        "                \"question\": question,\n",
        "                \"generated_answer\": generated_answer,\n",
        "                \"original_answer\": original_answer,\n",
        "                \"status\": \"success\"\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Generation error: {str(e)}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "            results.append({\n",
        "                \"question_id\": i,\n",
        "                \"question\": question,\n",
        "                \"generated_answer\": f\"Error: {str(e)}\",\n",
        "                \"original_answer\": original_answer,\n",
        "                \"status\": \"error\"\n",
        "            })\n",
        "\n",
        "    if results:\n",
        "        with open('/content/adapter_test_results.json', 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "        successful_results = [r for r in results if r['status'] == 'success']\n",
        "        print(f\"Success rate: {len(successful_results)}/{len(results)}\")\n",
        "\n",
        "        from google.colab import files\n",
        "        files.download('/content/adapter_test_results.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "collapsed": true,
        "id": "29FtTSyM7x9e",
        "outputId": "f81c82e2-5624-446c-e4b9-7abc57613369"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading questions from ./QA_finetuning_test/sample_training_data.json...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting property name enclosed in double quotes: line 101 column 1 (char 10383)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-4103931227.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtest_questions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_questions_from_sample_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-4103931227.py\u001b[0m in \u001b[0;36mload_questions_from_sample_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found {len(data)} conversations in sample data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 101 column 1 (char 10383)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CY5MX9_NBWCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-rB38tQBWFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a18a070a",
        "outputId": "5634070f-8e7a-430a-8efd-cf40d2a403d7"
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# seeing contents of json file\n",
        "\n",
        "sample_file = \"./QA_finetuning_test/sample_training_data.json\"\n",
        "\n",
        "if os.path.exists(sample_file):\n",
        "    print(f\"Content of {sample_file}:\")\n",
        "    with open(sample_file, 'r') as f:\n",
        "        print(f.read())\n",
        "else:\n",
        "    print(f\"File not found: {sample_file}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content of ./QA_finetuning_test/sample_training_data.json:\n",
            "[\n",
            "  {\n",
            "    \"messages\": [\n",
            "      {\n",
            "        \"role\": \"system\",\n",
            "        \"content\": \"You are an expert in symbolic computation and polynomial decomposition.\\nYour task is to help rewrite a target polynomial into the required form based on given inequality premises.\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": \"Consider the following constraints:\\ng_1 = (z - 2) >= 0\\ng_2 = (y + 2*z - 3) >= 0\\ng_3 = (2*y + z) >= 0\\ng_4 = (y^2*z^2 - 2*y*z - 2) >= 0\\ng_5 = (z + 2) >= 0\\n\\nConsider the target polynomial:\\nf = y*z^3 + 6*y*z^2 + 12*y*z + 11*y + 2*z^4 + 9*z^3 + 6*z^2 - 14*z - 33\\n\\nOur objective is to rewrite f in the following form:\\nf = C_1 * h_1 * s_1 + C_2 * h_2 * s_2 + ... + C_m * h_m * s_m\\nwhere:\\nC_i is a positive constant,\\nh_i is the product of a subset of g_j, and\\ns_i is a square of a polynomial.\\n\\nWe plan to work it out in the following steps:\\n1. Decompose the terms and introduce new terms if necessary.\\n2. Rearrange the terms and group them by adding parentheses.\\n3. Factor each group using sympy.factor().\\n4. Check if each group matches the desired form C_i * h_i * s_i.\\n\\nLet us start with Step 1. Please output the expression only.\\n\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"y*z^3 + 6*y*z^2 + 12*y*z + 8*y + 3*y + 2*z^4 + 9*z^3 + 6*z^2 - 20*z + 6*z - 24 - 9\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": \"Step 2:\\nHere is the current expression:\\ny*z^3 + 6*y*z^2 + 12*y*z + 8*y + 3*y + 2*z^4 + 9*z^3 + 6*z^2 - 20*z + 6*z - 24 - 9\\n\\nRearrange the terms and group them by adding parentheses.\\nOutput the expression only.\\n\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"(y*z^3 + 6*y*z^2 + 12*y*z + 8*y + 2*z^4 + 9*z^3 + 6*z^2 - 20*z - 24) + (3*y + 6*z - 9)\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": \"Step 3: Factor each group using sympy.factor().\\nHere is the current expression:\\n(y*z^3 + 6*y*z^2 + 12*y*z + 8*y + 2*z^4 + 9*z^3 + 6*z^2 - 20*z - 24) + (3*y + 6*z - 9)\\n\\nYou should give the code following the instructions:\\n1. Identify each group in the expression\\n2. Apply sympy.factor() to each group individually\\n3. Convert the factored results back to string format; Replace ** by ^\\n4. Enclose each factored group in square brackets []\\n5. Concatenate the all factored groups with symbol\\\"+\\\" to form the final expression\\n\\nOutput only the Python code for the factor, enclosed within the code wrapper <<< >>>.\\n\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"<<<str([sympy.factor('y*z^3+6*y*z^2+12*y*z+8*y+2*z^4+9*z^3+6*z^2-20*z-24')]).replace('**','^') + ' + ' + str([sympy.factor('3*y+6*z-9')]).replace('**','^')>>>\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": \"Step 4: Check if each group matches the desired form C_i * h_i * s_i.\\n        \\nHere is the output from previous python code:\\n[(z + 2)^3*(y + 2*z - 3)] + [3*(y + 2*z - 3)]\\n\\nRecall the premises:\\ng_1 = (z - 2) >= 0\\ng_2 = (y + 2*z - 3) >= 0\\ng_3 = (2*y + z) >= 0\\ng_4 = (y^2*z^2 - 2*y*z - 2) >= 0\\ng_5 = (z + 2) >= 0\\n\\nPlease check if each term (wrapped by []) can be rewritten in the following form\\n    C_i * h_i * s_i\\nwhere:\\nC_i is a positive constant,\\nh_i is a product of a subset of g_j, and\\ns_i is a square of a polynomial.\\n\\nIf it is possible, please output the rewritten expression. Otherwise, please output <<<Fail>>>.\\n\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"[((y + 2*z - 3))*((z + 2))*(z + 2)^2] + [3*((y + 2*z - 3))]\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"messages\": [\n",
            "      {\n",
            "        \"role\": \"system\",\n",
            "        \"content\": \"You are an expert in symbolic computation and polynomial decomposition.\\nYour task is to help rewrite a target polynomial into the required form based on given inequality premises.\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": \"Consider the following constraints:\\ng_1 = (x*z + 2) >= 0\\ng_2 = (z + 1) >= 0\\ng_3 = (z*(x - 3)) >= 0\\n\\nConsider the target polynomial:\\nf = 8*x^6*z^3 + 8*x^6*z^2 - 8*x^5*z^4 - 8*x^5*z^3 - 48*x^5*z^2 - 48*x^5*z + 2*x^4*z^5 + 2*x^4*z^4 + 48*x^4*z^3 + 48*x^4*z^2 + 72*x^4*z + 72*x^4 + 5*x^3*z^8 + 25*x^3*z^7 + 40*x^3*z^6 + 20*x^3*z^5 - 12*x^3*z^4 - 12*x^3*z^3 - 72*x^3*z^2 - 72*x^3*z + 30*x^2*z^7 + 150*x^2*z^6 + 240*x^2*z^5 + 120*x^2*z^4 + 18*x^2*z^3 + 18*x^2*z^2 + 60*x*z^6 + 300*x*z^5 + 480*x*z^4 + 240*x*z^3 + 40*z^5 + 200*z^4 + 320*z^3 + 160*z^2\\n\\nOur objective is to rewrite f in the following form:\\nf = C_1 * h_1 * s_1 + C_2 * h_2 * s_2 + ... + C_m * h_m * s_m\\nwhere:\\nC_i is a positive constant,\\nh_i is the product of a subset of g_j, and\\ns_i is a square of a polynomial.\\n\\nWe plan to work it out in the following steps:\\n1. Decompose the terms and introduce new terms if necessary.\\n2. Rearrange the terms and group them by adding parentheses.\\n3. Factor each group using sympy.factor().\\n4. Check if each group matches the desired form C_i * h_i * s_i.\\n\\nLet us start with Step 1. Please output the expression only.\\n\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"8*x^6*z^3 + 8*x^6*z^2 - 8*x^5*z^4 - 8*x^5*z^3 - 48*x^5*z^2 - 48*x^5*z + 2*x^4*z^5 + 2*x^4*z^4 + 48*x^4*z^3 + 48*x^4*z^2 + 72*x^4*z + 72*x^4 + 5*x^3*z^8 + 25*x^3*z^7 + 40*x^3*z^6 + 20*x^3*z^5 - 12*x^3*z^4 - 12*x^3*z^3 - 72*x^3*z^2 - 72*x^3*z + 30*x^2*z^7 + 150*x^2*z^6 + 240*x^2*z^5 + 120*x^2*z^4 + 18*x^2*z^3 + 18*x^2*z^2 + 60*x*z^6 + 300*x*z^5 + 480*x*z^4 + 240*x*z^3 + 40*z^5 + 200*z^4 + 320*z^3 + 160*z^2\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": \"Step 2:\\nHere is the current expression:\\n8*x^6*z^3 + 8*x^6*z^2 - 8*x^5*z^4 - 8*x^5*z^3 - 48*x^5*z^2 - 48*x^5*z + 2*x^4*z^5 + 2*x^4*z^4 + 48*x^4*z^3 + 48*x^4*z^2 + 72*x^4*z + 72*x^4 + 5*x^3*z^8 + 25*x^3*z^7 + 40*x^3*z^6 + 20*x^3*z^5 - 12*x^3*z^4 - 12*x^3*z^3 - 72*x^3*z^2 - 72*x^3*z + 30*x^2*z^7 + 150*x^2*z^6 + 240*x^2*z^5 + 120*x^2*z^4 + 18*x^2*z^3 + 18*x^2*z^2 + 60*x*z^6 + 300*x*z^5 + 480*x*z^4 + 240*x*z^3 + 40*z^5 + 200*z^4 + 320*z^3 + 160*z^2\\n\\nRearrange the terms and group them by adding parentheses.\\nOutput the expression only.\\n\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"(8*x^6*z^3 + 8*x^6*z^2 - 8*x^5*z^4 - 8*x^5*z^3 - 48*x^5*z^2 - 48*x^5*z + 2*x^4*z^5 + 2*x^4*z^4 + 48*x^4*z^3 + 48*x^4*z^2 + 72*x^4*z + 72*x^4 - 12*x^3*z^4 - 12*x^3*z^3 - 72*x^3*z^2 - 72*x^3*z + 18*x^2*z^3 + 18*x^2*z^2) + (5*x^3*z^8 + 25*x^3*z^7 + 40*x^3*z^6 + 20*x^3*z^5 + 30*x^2*z^7 + 150*x^2*z^6 + 240*x^2*z^5 + 120*x^2*z^4 + 60*x*z^6 + 300*x*z^5 + 480*x*z^4 + 240*x*z^3 + 40*z^5 + 200*z^4 + 320*z^3 + 160*z^2)\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": \"Step 3: Factor each group using sympy.factor().\\nHere is the current expression:\\n(8*x^6*z^3 + 8*x^6*z^2 - 8*x^5*z^4 - 8*x^5*z^3 - 48*x^5*z^2 - 48*x^5*z + 2*x^4*z^5 + 2*x^4*z^4 + 48*x^4*z^3 + 48*x^4*z^2 + 72*x^4*z + 72*x^4 - 12*x^3*z^4 - 12*x^3*z^3 - 72*x^3*z^2 - 72*x^3*z + 18*x^2*z^3 + 18*x^2*z^2) + (5*x^3*z^8 + 25*x^3*z^7 + 40*x^3*z^6 + 20*x^3*z^5 + 30*x^2*z^7 + 150*x^2*z^6 + 240*x^2*z^5 + 120*x^2*z^4 + 60*x*z^6 + 300*x*z^5 + 480*x*z^4 + 240*x*z^3 + 40*z^5 + 200*z^4 + 320*z^3 + 160*z^2)\\n\\nYou should give the code following the instructions:\\n1. Identify each group in the expression\\n2. Apply sympy.factor() to each group individually\\n3. Convert the factored results back to string format; Replace ** by ^\\n4. Enclose each factored group in square brackets []\\n5. Concatenate the all factored groups with symbol\\\"+\\\" to form the final expression\\n\\nOutput only the Python code for the factor, enclosed within the code wrapper <<< >>>.\\n\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"<<<str([sympy.factor('8*x^6*z^3+8*x^6*z^2-8*x^5*z^4-8*x^5*z^3-48*x^5*z^2-48*x^5*z+2*x^4*z^5+2*x^4*z^4+48*x^4*z^3+48*x^4*z^2+72*x^4*z+72*x^4-12*x^3*z^4-12*x^3*z^3-72*x^3*z^2-72*x^3*z+18*x^2*z^3+18*x^2*z^2')]).replace('**','^') + ' + ' + str([sympy.factor('5*x^3*z^8+25*x^3*z^7+40*x^3*z^6+20*x^3*z^5+30*x^2*z^7+150*x^2*z^6+240*x^2*z^5+120*x^2*z^4+60*x*z^6+300*x*z^5+480*x*z^4+240*x*z^3+40*z^5+200*z^4+320*z^3+160*z^2')]).replace('**','^')>>>\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": \"Step 4: Check if each group matches the desired form C_i * h_i * s_i.\\n        \\nHere is the output from previous python code:\\n[2*x^2*(2*x - z)^2*(z + 1)*(x*z - 3)^2] + [5*z^2*(z + 1)*(z + 2)^2*(x*z + 2)^3]\\n\\nRecall the premises:\\ng_1 = (x*z + 2) >= 0\\ng_2 = (z + 1) >= 0\\ng_3 = (z*(x - 3)) >= 0\\n\\nPlease check if each term (wrapped by []) can be rewritten in the following form\\n    C_i * h_i * s_i\\nwhere:\\nC_i is a positive constant,\\nh_i is a product of a subset of g_j, and\\ns_i is a square of a polynomial.\\n\\nIf it is possible, please output the rewritten expression. Otherwise, please output <<<Fail>>>.\\n\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"[2*((z + 1))*x^2*(x*z - 3)^2*(2*x - z)^2] + [5*((x*z + 2))*((z + 1))*(x*z + 2)^2*z^2*(z + 2)^2]\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"messages\": [\n",
            "      {\n",
            "        \"role\": \"system\",\n",
            "        \"content\": \"You are an expert in symbolic computation and polynomial decomposition.\\nYour task is to help rewrite a target polynomial into the required form based on given inequality premises.\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": \"Consider the following constraints:\\ng_1 = (3*x^2 + 2*x + 2) >= 0\\ng_2 = (4*z^2) >= 0\\ng_3 = (x^2*z + x - 2) >= 0\\n\\nConsider the target polynomial:\\nf = 2*x^4*z^2 + 4*x^3*z - 8*x^2*z + 2*x^2 - 8*x + 8\\n\\nOur objective is to rewrite f in the following form:\\nf = C_1 * h_1 * s_1 + C_2 * h_2 * s_2 + ... + C_m * h_m * s_m\\nwhere:\\nC_i is a positive constant,\\nh_i is the product of a subset of g_j, and\\ns_i is a square of a polynomial.\\n\\nWe plan to work it out in the following steps:\\n1. Decompose the terms and introduce new terms if necessary.\\n2. Rearrange the terms and group them by adding parentheses.\\n3. Factor each group using sympy.factor().\\n4. Check if each group matches the desired form C_i * h_i * s_i.\\n\\nLet us start with Step 1. Please output the expression only.\\n\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"2*x^4*z^2 + 4*x^3*z - 8*x^2*z + 2*x^2 - 8*x + 8\"\n",
            "      },\n",
            "      {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": \"Step 2:\\nHere is the current expression:\\n2*x^4*z^2 + 4*x^3*z - 8*x^2*z + 2*x^2 - 8*x + 8\\n\\nRearrange the terms and group them by adding parentheses.\\nOutput the expression only.\\n\"\n",
            "      },\n",
            "      {\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7776bb8d",
        "outputId": "170581fd-b3da-4873-b816-f51d6b28491c"
      },
      "source": [
        "def generate_answer(model, tokenizer, question):\n",
        "    \"\"\"Generate answer for a given question\"\"\"\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "You are an expert in symbolic computation and polynomial decomposition.\n",
        "Your task is to help rewrite a target polynomial into the required form based on given inequality premises.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{question}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.3,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = response.split(\"<|im_start|>assistant\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "if 'model' in locals() and 'tokenizer' in locals() and model_loaded:\n",
        "    # example question for model\n",
        "    question = \"What is the simplified form of the expression (x+y)^2?\"\n",
        "\n",
        "    print(f\"Asking the model: {question}\")\n",
        "\n",
        "    try:\n",
        "        generated_answer = generate_answer(model, tokenizer, question)\n",
        "        print(f\"\\nModel's Answer:\\n{generated_answer}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during generation: {str(e)}\")\n",
        "\n",
        "else:\n",
        "    print(\"Model was not loaded successfully. Cannot generate an answer.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was not loaded successfully. Cannot generate an answer.\n"
          ]
        }
      ]
    }
  ]
}